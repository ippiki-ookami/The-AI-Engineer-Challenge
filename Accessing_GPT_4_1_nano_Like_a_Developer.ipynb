{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-Bn7FAeydrexcWC3Fo2MB3ia9J0rAO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular tools in the realm of building applications with language models, but they serve different purposes and have distinct features. Here's a breakdown of their differences:\\n\\n**1. Purpose and Use Cases**\\n\\n- **LangChain:**  \\n  - **Primary Focus:** Framework for building large language model (LLM)-powered applications with a focus on chaining multiple calls, memory management, and complex workflows.  \\n  - **Use Cases:** Chatbots, virtual assistants, question-answering systems, tools integrating multiple APIs or data sources, conversational agents, and more advanced LLM applications involving sequences of operations.\\n\\n- **LlamaIndex (GPT Index):**  \\n  - **Primary Focus:** Facilitates easy indexing, querying, and retrieval of data from large document repositories using LLMs.  \\n  - **Use Cases:** Building custom document retrieval systems, knowledge bases, semantic search engines, and other data-centric AI applications that require efficient document indexing and querying.\\n\\n**2. Core Functionality**\\n\\n- **LangChain:**  \\n  - Provides abstractions for prompt management, memory, agents, and chains to chain together various LLM calls.  \\n  - Supports integration with multiple LLM providers, tools, and APIs.  \\n  - Enables complex, multi-step workflows including decision-making, external API calls, and dynamic reasoning.\\n\\n- **LlamaIndex:**  \\n  - Focuses on creating indexes over large unstructured data (e.g., PDFs, documents, webpages).  \\n  - Offers data ingestion pipelines, vector stores, and retrieval methods optimized for querying large document sets.  \\n  - Simplifies the process of embedding and retrieving relevant chunks of data based on user queries.\\n\\n**3. Architecture and Design**\\n\\n- **LangChain:**  \\n  - Modular and composable architecture emphasizing chains, tools, memory, and agents.  \\n  - Code-centric approach encouraging developers to design complex interactions and workflows with minimal boilerplate.\\n\\n- **LlamaIndex:**  \\n  - Data-centric approach centered around creating and querying indexes from unstructured data sources.  \\n  - Focused on efficient retrieval-augmented generation (RAG) techniques, often used to supplement LLM prompts with relevant data.\\n\\n**4. Integration and Extensibility**\\n\\n- **LangChain:**  \\n  - Supports multiple LLM providers and enables building custom chains, tools, and agents.  \\n  - Extensible with custom components, prompt templates, and external API integrations.\\n\\n- **LlamaIndex:**  \\n  - Designed for data ingestion pipelines and flexible in handling various document formats.  \\n  - Can be integrated with LLMs for querying, but less focused on workflow orchestration.\\n\\n---\\n\\n### In summary:\\n\\n| Aspect | **LangChain** | **LlamaIndex** |\\n|---|---|---|\\n| **Primary focus** | Building complex LLM applications, workflows, and integrations | Efficient document indexing and retrieval for data-driven queries |\\n| **Use cases** | Chatbots, multi-step workflows, tools, agents | Knowledge bases, semantic search, document Q&A |\\n| **Architecture** | Modular chaining, memory, tools | Indexing, embedding, retrieval systems |\\n| **Integration** | Multiple LLM providers, custom workflows | Data ingestion and retrieval from large datasets |\\n\\n---\\n\\n### Conclusion:\\nChoose **LangChain** if you're looking to build sophisticated, multi-step AI applications with custom workflows, memory, and tool integrations. Opt for **LlamaIndex** if your primary goal is to organize, index, and efficiently retrieve information from large document collections to answer queries or support data-driven applications.\\n\\nIf you're combining both, they can often be used together: LlamaIndex to handle data retrieval and LangChain to manage conversational workflows and integrations.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1751045616, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=757, prompt_tokens=19, total_tokens=776, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of applications leveraging large language models (LLMs), but they serve different purposes and have distinct features. Here's a comparison to clarify their differences:\n",
              "\n",
              "### Purpose and Focus\n",
              "- **LangChain**:\n",
              "  - Primarily a framework for building **decentralized, composable applications** with LLMs.\n",
              "  - Focuses on **prompt management, model chaining, memory, and agent construction**.\n",
              "  - Facilitates complex workflows, including conversation management, tools integration, and multi-step reasoning.\n",
              "\n",
              "- **LlamaIndex (GPT Index)**:\n",
              "  - Designed to **connect LLMs with external data sources**, especially large datasets or documents.\n",
              "  - Focuses on **indexing and querying unstructured data** like text documents, PDFs, or knowledge bases.\n",
              "  - Simplifies building **question-answering systems**, retrieval, and information retrieval from data sources.\n",
              "\n",
              "### Core Capabilities\n",
              "- **LangChain**:\n",
              "  - Tool and API for chaining multiple prompts.\n",
              "  - Supports memory management for stateful conversations.\n",
              "  - Integrates with multiple LLM providers.\n",
              "  - Supports agents that can choose actions based on context.\n",
              "  - Facilitates complex workflows like summarization, translation, or chain-of-thought reasoning.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Provides indexing methods to convert raw data into queryable formats.\n",
              "  - Offers pre-built data loaders for various file types.\n",
              "  - Supports retrieval-augmented generation (RAG) workflows.\n",
              "  - Enables efficient querying over large document collections and knowledge bases.\n",
              "\n",
              "### Use Cases\n",
              "- **LangChain**:\n",
              "  - Building chatbots, conversational agents.\n",
              "  - Multi-step reasoning pipelines.\n",
              "  - Applications requiring tool integration and decision-making.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Building knowledge bases.\n",
              "  - Document search and retrieval.\n",
              "  - Enhanced question-answering over large datasets.\n",
              "\n",
              "### Integration and Extensibility\n",
              "- **LangChain**:\n",
              "  - Highly modular with support for many prompt templates, memory modules, and agent types.\n",
              "  - Extensive integrations with LLM providers, chat models, and tools.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Focuses on data ingestion, indexing, and querying.\n",
              "  - Can be combined with other frameworks for more complex workflows but is primarily structured around data management.\n",
              "\n",
              "---\n",
              "\n",
              "### In Summary\n",
              "| Aspect | **LangChain** | **LlamaIndex** |\n",
              "|---------|---------------|----------------|\n",
              "| Main Purpose | Building LLM-powered applications, workflows, and agents | Connecting LLMs with external data, indexing, and retrieval |\n",
              "| Focus | Workflow orchestration, chaining, tools, memory | Data indexing, retrieval, knowledge bases |\n",
              "| Use Cases | Conversational agents, multi-step processes | Document QA, knowledge management |\n",
              "| Strength | Modular, flexible workflow construction | Efficient data retrieval and querying |\n",
              "\n",
              "---\n",
              "\n",
              "### Conclusion\n",
              "While both frameworks can complement each other, **LangChain** is best suited for orchestrating complex LLM applications and workflows, whereas **LlamaIndex** excels at importing, indexing, and querying large datasets for use with LLMs.\n",
              "\n",
              "If you're building an application that involves both complex interaction logic and external data retrieval, you might consider using them together for a robust solution."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? Neither! I can't believe I have to choose between crushed ice and cubed ice while I'm this furious and starving! Just give me some good old-fashioned ice that won't ruin my mood or my stomach!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice has a fun, refreshing vibe—perfect for keeping drinks cool and adding a bit of flair! Cubed ice, on the other hand, looks sleek and melts more slowly, making it ideal for sipping cocktails or neat beverages. Honestly, I enjoy both depending on the moment! Which do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-Bn7FQ7HvMoeIWk8XjQSJd19YTqzQh', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice has a fun, refreshing vibe—perfect for keeping drinks cool and adding a bit of flair! Cubed ice, on the other hand, looks sleek and melts more slowly, making it ideal for sipping cocktails or neat beverages. Honestly, I enjoy both depending on the moment! Which do you prefer?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1751045632, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=64, prompt_tokens=30, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure! Here's a sentence using both words:\n",
              "\n",
              "\"During the quirky parade, a stimple clown and a falbean dancer captured everyone's attention with their unusual costumes.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench and the falbean are essential tools for assembling the machinery, as the falbean spins smoothly to tighten bolts easily."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze both options carefully.\n",
              "\n",
              "**Option 1: Fly + Bus**\n",
              "- Fly time: 3 hours\n",
              "- Bus time: 2 hours\n",
              "- Total travel time: 3 + 2 = 5 hours\n",
              "\n",
              "**Option 2: Teleporter + Bus**\n",
              "- Teleporter time: 0 hours\n",
              "- Bus time: 1 hour\n",
              "- Total travel time: 0 + 1 = 1 hour\n",
              "\n",
              "---\n",
              "\n",
              "### Important details:\n",
              "- **Current local time:** 1 PM\n",
              "- **Target arrival time:** Before 7 PM EDT\n",
              "\n",
              "**Note:** Since Billy is currently in San Francisco and wants to arrive before 7 PM EDT, we need to determine if he can still make it on time.\n",
              "\n",
              "---\n",
              "\n",
              "### Step 1: Clarify the time difference\n",
              "\n",
              "- San Francisco is in the Pacific Time Zone (PT).\n",
              "- Eastern Daylight Time (EDT) is 3 hours ahead of PT.\n",
              "\n",
              "### Step 2: Convert current local time to EDT\n",
              "\n",
              "- It's currently 1 PM PT.\n",
              "- In EDT, it is 4 PM (since EDT is 3 hours ahead).\n",
              "\n",
              "### Step 3: Determine the latest arrival time in local time\n",
              "\n",
              "- Arrival must be before 7 PM EDT.\n",
              "- Convert 7 PM EDT to PT:\n",
              "\n",
              "  \\[\n",
              "  7 \\text{ PM EDT } - 3 \\text{ hours } = 4 \\text{ PM PT}\n",
              "  \\]\n",
              "\n",
              "- So, Billy must arrive **by 4 PM PT**.\n",
              "\n",
              "### Step 4: Calculate the earliest possible arrival times for both options\n",
              "\n",
              "- Current time: 1 PM PT.\n",
              "- **Option 1: Fly + Bus (5 hours total)**\n",
              "  \n",
              "  Arrival time:\n",
              "\n",
              "  \\[\n",
              "  1 \\text{ PM} + 5 \\text{ hours } = 6 \\text{ PM PT}\n",
              "  \\]\n",
              "  \n",
              "  Which is after the 4 PM deadline, so **not feasible**.\n",
              "\n",
              "- **Option 2: Teleporter + Bus (1 hour total)**\n",
              "\n",
              "  Arrival time:\n",
              "\n",
              "  \\[\n",
              "  1 \\text{ PM} + 1 \\text{ hour } = 2 \\text{ PM PT}\n",
              "  \\]\n",
              "  \n",
              "  Which is well before 4 PM PT, so **definitely feasible**.\n",
              "\n",
              "---\n",
              "\n",
              "### **Conclusion:**\n",
              "\n",
              "Yes, **it does matter** which option Billy chooses.  \n",
              "- Taking the teleporter + bus allows him to arrive at 2 PM PT (before his deadline).  \n",
              "- Flying + bus would result in arriving at 6 PM PT, which is **after** the 4 PM PT deadline (due to the 7 PM EDT requirement).\n",
              "\n",
              "**Therefore, Billy should choose the teleporter + bus.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's carefully analyze Billy's options to determine whether the choice of travel method will affect his ability to arrive home before 7 PM EDT.\n",
              "\n",
              "**Step 1: Determine Billy's current local time and his deadline.**\n",
              "\n",
              "- Current local time: 1 PM\n",
              "- Deadline: 7 PM EDT\n",
              "\n",
              "**Step 2: Understand the travel options and their durations.**\n",
              "\n",
              "- **Option 1:** Fly (3 hours) + bus (2 hours) = total of 5 hours\n",
              "- **Option 2:** Teleporter (0 hours) + bus (1 hour) = total of 1 hour\n",
              "\n",
              "Note: The teleporter makes the initial travel instantaneous, but the bus still takes 1 hour.\n",
              "\n",
              "**Step 3: Calculate the arrival time for each option from current time.**\n",
              "\n",
              "- **Option 1:**\n",
              "\n",
              "  - Total travel time: 5 hours\n",
              "\n",
              "  - Arrival time = 1 PM + 5 hours = 6 PM local time\n",
              "\n",
              "- **Option 2:**\n",
              "\n",
              "  - Total travel time: 1 hour\n",
              "\n",
              "  - Arrival time = 1 PM + 1 hour = 2 PM local time\n",
              "\n",
              "**Step 4: Determine if Billy arrives before 7 PM EDT.**\n",
              "\n",
              "- For **Option 1**, arriving at 6 PM local time:\n",
              "\n",
              "  - Since Billy's goal is to arrive before 7 PM EDT, and he will arrive at 6 PM local time, we need to know how local time relates to EDT at that moment.\n",
              "\n",
              "- For **Option 2**, arriving at 2 PM local time:\n",
              "\n",
              "  - Same consideration applies.\n",
              "\n",
              "**Step 5: Consider the time zone difference.**\n",
              "\n",
              "- Billy is traveling from San Francisco, which is in Pacific Time (PT).\n",
              "\n",
              "- New York (Eastern Time - ET, which includes EDT) is 3 hours ahead of Pacific Time.\n",
              "\n",
              "- Therefore:\n",
              "\n",
              "  - When it's 1 PM in San Francisco, it's 4 PM in New York.\n",
              "\n",
              "- The deadline is 7 PM EDT, which corresponds to:\n",
              "\n",
              "  - 4 PM PT + 3 hours = 7 PM ET\n",
              "\n",
              "  - Equivalently, in Pacific Time: 4 PM PT\n",
              "\n",
              "**Step 6: Convert Billy's current time to EDT and assess arrival times.**\n",
              "\n",
              "- Billy's current local time: 1 PM PT\n",
              "\n",
              "- Convert to EDT: 4 PM EDT\n",
              "\n",
              "- **Option 1:** Arrives at 6 PM PT\n",
              "\n",
              "  - Corresponds to: 6 PM PT + 3 hours = 9 PM EDT\n",
              "\n",
              "- **Option 2:** Arrives at 2 PM PT\n",
              "\n",
              "  - Corresponds to: 2 PM PT + 3 hours = 5 PM EDT\n",
              "\n",
              "**Step 7: Check if Billy arrives before 7 PM EDT.**\n",
              "\n",
              "- **Option 1:** Arrival at ~9 PM EDT — **Not before the deadline**\n",
              "\n",
              "- **Option 2:** Arrival at ~5 PM EDT — **Before the deadline**\n",
              "\n",
              "**Conclusion:**\n",
              "\n",
              "- If Billy chooses **Option 1 (flying + bus)**, he arrives after 7 PM EDT (at 9 PM), missing his deadline.\n",
              "\n",
              "- If he chooses **Option 2 (teleporter + bus)**, he arrives well before 7 PM EDT.\n",
              "\n",
              "**Therefore, it **does matter** which option Billy chooses. To arrive on time, he should choose the teleporter + bus option.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
